---
title: "Team 5 - Data 624 - Project 2"
author: "Ohannes (Hovig) Ohannessian, Niteen Kumar, Gurpreet Singh, Peter Goodridge"
date: "4/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_packages}
library("readxl")
library(httr)
library(caret)
library(tidyverse)
library(kableExtra)
library(caretEnsemble)
library(mice)
library(kableExtra)
library(xgboost)
#library(parallel)
library(doParallel)
library(ggplot2)
library(Hmisc)
library(psych)
library(reshape2)
library(gridExtra)
library(rpart.plot)
suppressWarnings(suppressMessages(library(data.table)))
suppressWarnings(suppressMessages(library(openxlsx)))
suppressWarnings(suppressMessages(library(psych)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(DataExplorer)))
suppressWarnings(suppressMessages(library(VIM)))
suppressWarnings(suppressMessages(library(corrplot)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(PerformanceAnalytics)))
#suppressWarnings(suppressMessages(library(DMwR)))
```


```{r}
student_evaluation_gh_file <- "https://github.com/hovig/Team5-Data624-Project2/raw/master/StudentEvaluation.xlsx"
student_data_gh_file <- "https://github.com/hovig/Team5-Data624-Project2/raw/master/StudentData.xlsx"

se_temp_file <- tempfile(fileext = ".xlsx")
sd_temp_file <- tempfile(fileext = ".xlsx")

se_data <- GET(student_evaluation_gh_file, authenticate(Sys.getenv("GITHUB_PAT"), ""), write_disk(path = se_temp_file))
df_data <- GET(student_data_gh_file, authenticate(Sys.getenv("GITHUB_PAT"), ""), write_disk(path = sd_temp_file))

se_data <- readxl::read_excel(se_temp_file)
sd_data <- readxl::read_excel(sd_temp_file)

```

```{r}
hist.data.frame(se_data)
```


```{r}
hist.data.frame(sd_data)
```


####Process Data

```{r}
bev_raw <- read_csv('https://raw.githubusercontent.com/hovig/Team5-Data624-Project2/master/StudentData.csv') #%>%
#drop_na(PH)
```

```{r}
bev_raw %>%
  ggplot(aes(PH, fill=PH > 9)) + 
  geom_histogram(bins=30) +
  theme_bw() +
  theme(legend.position='center') +
  labs(y='Count', title='PH Levels in Dataset')

bev_raw <- bev_raw %>% filter(!is.na(bev_raw$PH), bev_raw$PH < 9) 
```

```{r}
dim(bev_raw)
str(bev_raw)
hist.data.frame(bev_raw)
table(bev_raw$`Brand Code`)
summary(bev_raw)
describe(bev_raw %>% select(-`Brand Code`))
```


#### Zero variance

```{r}
df1 <- bev_raw %>% select(-`Brand Code`) %>% mutate_each(funs(as.numeric(.)))%>%complete.cases()%>%
       as.data.frame()


names(df1)[nearZeroVar(df1)]
nzv <- nearZeroVar(bev_raw,saveMetrics= TRUE)
nzv[nzv$nzv,]
```


#### Box plot

```{r}
df.m <- melt(bev_raw %>% select(-MFR, -`Filler Speed`, -`Carb Flow`,-`Bowl Setpoint`,`Carb Pressure1`,
            `Hyd Pressure4`, `Air Pressurer`, `Carb Temp`, `Filler Level`, `Mnf Flow`), id.var = "Brand Code")
p <-ggplot(data = df.m, aes(x=variable, y=value)) + 
  geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4,aes(fill=variable)) +     
  scale_y_continuous(name = "Predictors for PH", breaks = seq(0, 2, 0.5))  + 
  coord_flip()
p
```

### Normality 

```{r}
bev_raw%>%
 select(-`Brand Code`) %>%   
  select(2:20) %>%         
  gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density()  
bev_raw%>%
 select(-`Brand Code`) %>%   
  select(`Carb Flow`, Density, MFR, Balling, `Pressure Vacuum`, PH, `Oxygen Filler`, `Bowl Setpoint`, `Pressure Setpoint`, `Air Pressurer`, `Alch Rel`,`Carb Rel`,`Balling Lvl`)   %>%               
  gather() %>%                             
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density() 
```

###Prep Data

We perform 3 data preperatoin steps
- Remove the near-zero variance variables we previously mentioned
- Impute missing values with a Random Forest Regression with the MICE package
- Create dummy variables for the categorical variable, brand

Random Forest was chosen as the regression method in imputation because it requires very little tuning, allowing and there is no interface to tune the imputation model in MICE.

```{r results='hide'}
bev <- select(bev_raw, -PH)
cols <- str_replace(colnames(bev), '\\s', '')
colnames(bev) <- cols
bev <- mutate(bev, BrandCode = ifelse(is.na(BrandCode), 'Unknown', BrandCode))
y <- bev_raw$PH

zero_vars <- nearZeroVar(bev)
bev_new <- bev[, -zero_vars]

pred <- mice(data = bev_new, m = 2, method = 'rf', maxit = 3)
bev_imputed <- complete(pred)

form <- dummyVars(~ ., data = bev_imputed)
bev_imputed <- predict(form, bev_imputed) %>% data.frame() %>% as.matrix()
write.csv(bev_imputed, file = "predictions.csv")
```

####Test Train Split

```{r}
samples <- createDataPartition(y, p = .75, list = F)
x_train <- bev_imputed[samples, ]
x_test<- bev_imputed[-samples, ]

y_train <- y[samples]
y_test <- y[-samples]
```

###Build Models

For state-of-the-art prediction quality, we will use a model stack.  This will consist of tuning models separately and then combining the candidate models into a metamodel that will formulate predictions with a linear combination of our tuned models.  

From the perspective of understanding the manufacturing process, the model stack will also provide benefits.  The stack is like a panel of experts, each looking at the data through slightly different lenses to form their diagnoses.  By looking at the predictors each model uses, we can gather assemble a complete picture of the factors that affect our manufacturing process.  


```{r}
#xgb_grid <- expand.grid(eta = c(.025, .05), nrounds = c(1000), max_depth = c(5,6,7), gamma = c(0), colsample_bytree = c(.6), min_child_weight = c(.6,1), subsample = c(.6))
#cub_grid <- expand.grid(.committees = c(1,3,5), .neighbors = c(1,3,5,7,9))
#mars_grid <- expand.grid(.degree = c(1,2), .nprune = seq(16,36,4))
#dart_grid <- expand.grid(eta = c(.025), nrounds = c(1000), gamma = c(.001,.1), skip_drop = c(.4,.6), rate_drop = c(.4,.6), max_depth = c(6,7,8), colsample_bytree = c(.6), min_child_weight = c(.6), subsample = c(.6))

xgb_grid <- expand.grid(eta = c(.025), nrounds = c(1000), max_depth = c(5), gamma = c(0), colsample_bytree = c(.6), min_child_weight = c(.6), subsample = c(.6))
cub_grid <- expand.grid(.committees = c(5), .neighbors = c(7))
mars_grid <- expand.grid(.degree = c(2), .nprune = 24)
dart_grid <- expand.grid(eta = c(.025), nrounds = c(1000), gamma = c(.1), skip_drop = c(.6), rate_drop = c(.4), max_depth = c(6), colsample_bytree = c(.6), min_child_weight = c(.6), subsample = c(.6))
rf_grid <- expand.grid(mtry = 25)

tuning_list <-list(
  caretModelSpec(method="xgbTree", tuneGrid = xgb_grid),
  caretModelSpec(method="xgbDART", tuneGrid = dart_grid),
  caretModelSpec(method="cubist", tuneGrid = cub_grid),
  caretModelSpec(method="rf", tuneGrid = rf_grid, importance = TRUE),
  caretModelSpec(method="bagEarth", tuneGrid = mars_grid)
)

cl <- makeCluster(detectCores())
registerDoParallel(cl)
set.seed(42)

my_control <- trainControl(method = 'cv',
                           number = 5,
                           savePredictions = 'final',
                           index = createFolds(y_train, 5),
                           allowParallel = TRUE) 

mod_list <- caretList(
  x = x_train,
  y = y_train,
  preProcess = c('center', 'scale'),
  trControl = my_control,
  tuneList = tuning_list
)
```


####MARS

```{r }
#helper function
show_results <- function(model){ 
  rslts <- model$results %>%
    arrange(RMSE)
  head(rslts, 10) %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)
}
```

```{r}

show_results(mod_list$bagEarth)
#plot(mod_list$mars, metric = "RMSE")
```


```{r}
predict(mod_list$bagEarth, x_test) %>%
  postResample(y_test)
```

####RandomForest

```{r}
show_results(mod_list$rf)

```


```{r}
predict(mod_list$rf, x_test) %>%
  postResample(y_test)
```

####Cubist

```{r}
show_results(mod_list$cubist)
#plot(mod_list$cub, metric = "RMSE")
```


```{r}
predict(mod_list$cubist, x_test) %>%
  postResample(y_test)
```

####XGB Trees

```{r}
  
show_results(mod_list$xgbTree)
#plot(mod_list$xgbt, metric = "RMSE")
```


```{r}
predict(mod_list$xgbTree, x_test) %>%
  postResample(y_test)
```

####XGB Dart

```{r}
show_results(mod_list$xgbDART)
#plot(mod_list$xgbd, metric = "RMSE")
```

```{r}
predict(mod_list$xgbDART, x_test) %>%
  postResample(y_test)
```

####Variable Importances

```{r}
p1 <- varImp(mod_list$cubist) %>%
  plot(top = 10, main = 'Cubist')
p2 <- varImp(mod_list$xgbTree) %>%
  plot(top = 10, main = 'XGBTrees')
p3 <- varImp(mod_list$xgbDART) %>%
  plot(top = 10, main = 'XGBDart')
p4 <- varImp(mod_list$bagEarth) %>%
  plot(top = 10, main = 'MARS')
p5 <- varImp(mod_list$rf) %>%
  plot(top = 10, main = 'RF')
grid.arrange(p1,p2,p3,p4,p5, nrow = 3, ncol = 2)
```

**Variable Counts among the Models**

```{r}
top_10 <- function(model) {
  varImp(model)$importance %>%
  rownames_to_column('var') %>%
  arrange(desc(Overall)) %>%
  head(10) %>%
  select(var) 
}

rbind(top_10(mod_list$xgbDART), top_10(mod_list$xgbTree), top_10(mod_list$cubist), top_10(mod_list$bagEarth), top_10(mod_list$rf)) %>%
  group_by(var) %>%
  summarise(ModelCount = n()) %>%
  arrange(desc(ModelCount)) %>%
  kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)

```

**Best Single Tree**

```{r}
rp_mod <- rpart::rpart(ph ~ ., data = data.frame(bev_imputed, ph = y), method = 'anova')
rpart.plot(rp_mod, uniform=TRUE)
```

####Test Stack

```{r}

resamples <- resamples(mod_list)
modelCor(resamples)
```


```{r}
ensemble1 <- caretStack(
  mod_list,
  method="glm",
  metric="RMSE",
  trControl=trainControl(
    method="cv",
    number=5,
    savePredictions="final"
  )
)
ensemble1
```
```{r}
predict(ensemble1, x_test) %>%
  postResample(y_test)
```

####Predict on Eval Set

