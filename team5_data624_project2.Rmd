---
title: "Team 5 - Data 624 - Project 2"
author: "Ohannes (Hovig) Ohannessian, Niteen Kumar, Gurpreet Singh, Peter Goodridge"
date: "4/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

##Initialize and Load Data

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_packages}
library("readxl")
library(httr)
library(caret)
library(tidyverse)
library(kableExtra)
library(caretEnsemble)
library(mice)
library(kableExtra)
library(xgboost)
#library(parallel)
library(doParallel)
library(ggplot2)
library(Hmisc)
library(psych)
library(reshape2)
library(gridExtra)
library(rpart.plot)
library(DT)
suppressWarnings(suppressMessages(library(data.table)))
suppressWarnings(suppressMessages(library(openxlsx)))
suppressWarnings(suppressMessages(library(psych)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(DataExplorer)))
suppressWarnings(suppressMessages(library(VIM)))
suppressWarnings(suppressMessages(library(corrplot)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(PerformanceAnalytics)))
#suppressWarnings(suppressMessages(library(DMwR)))
```


```{r}
student_evaluation_gh_file <- "https://github.com/hovig/Team5-Data624-Project2/raw/master/StudentEvaluation.xlsx"

se_temp_file <- tempfile(fileext = ".xlsx")

se_data <- GET(student_evaluation_gh_file, authenticate(Sys.getenv("GITHUB_PAT"), ""), write_disk(path = se_temp_file))

se_data <- readxl::read_excel(se_temp_file)

bev_raw <- read_csv('https://raw.githubusercontent.com/hovig/Team5-Data624-Project2/master/StudentData.csv') #%>%
#drop_na(PH)
```


##Explore and Process Data


```{r}
bev_raw %>%
  ggplot(aes(PH, fill=PH > 9)) + 
  geom_histogram(bins=30) +
  theme_bw() +
  theme(legend.position='center') +
  labs(y='Count', title='PH Levels in Dataset')

bev_raw <- bev_raw %>% filter(!is.na(bev_raw$PH), bev_raw$PH < 9) 
```

```{r}
dim(bev_raw)
str(bev_raw)
hist.data.frame(bev_raw)
table(bev_raw$`Brand Code`)
summary(bev_raw)
describe(bev_raw %>% select(-`Brand Code`))
```


###Zero variance

```{r}
df1 <- bev_raw %>% select(-`Brand Code`) %>% mutate_each(funs(as.numeric(.)))%>%complete.cases()%>%
       as.data.frame()


names(df1)[nearZeroVar(df1)]
nzv <- nearZeroVar(bev_raw,saveMetrics= TRUE)
nzv[nzv$nzv,]
```


###Box plot

```{r}
df.m <- melt(bev_raw %>% select(-MFR, -`Filler Speed`, -`Carb Flow`,-`Bowl Setpoint`,`Carb Pressure1`,
            `Hyd Pressure4`, `Air Pressurer`, `Carb Temp`, `Filler Level`, `Mnf Flow`), id.var = "Brand Code")
p <-ggplot(data = df.m, aes(x=variable, y=value)) + 
  geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4,aes(fill=variable)) +     
  scale_y_continuous(name = "Predictors for PH", breaks = seq(0, 2, 0.5))  + 
  coord_flip()
p
```

###Normality 

```{r}
bev_raw%>%
 select(-`Brand Code`) %>%   
  select(2:20) %>%         
  gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density()  
bev_raw%>%
 select(-`Brand Code`) %>%   
  select(`Carb Flow`, Density, MFR, Balling, `Pressure Vacuum`, PH, `Oxygen Filler`, `Bowl Setpoint`, `Pressure Setpoint`, `Air Pressurer`, `Alch Rel`,`Carb Rel`,`Balling Lvl`)   %>%               
  gather() %>%                             
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density() 
```

##Prep Data

We perform 3 data preparation steps
- Remove the near-zero variance variables we previously mentioned
- Impute missing values with a Random Forest Regression with the MICE package
- Create dummy variables for the categorical variable, brand

Random Forest was chosen as the regression method in imputation because it requires very little tuning, allowing and there is no interface to tune the imputation model in MICE.

```{r results='hide'}
bev <- select(bev_raw, -PH) %>%
  union_all(se_data) #since we're not using the labels (y values), combining the sets for data clearning is fine
cols <- str_replace(colnames(bev), '\\s', '')
colnames(bev) <- cols
bev <- mutate(bev, BrandCode = ifelse(is.na(BrandCode), 'Unknown', BrandCode))
y <- bev_raw$PH

zero_vars <- nearZeroVar(bev[1:2566, ])
bev_new <- bev[, -zero_vars]

pred <- mice(data = bev_new, m = 3, method = 'rf', maxit = 3)
bev_imputed <- complete(pred)

form <- dummyVars(~ ., data = bev_imputed)
bev_imputed <- predict(form, bev_imputed) %>% data.frame() %>% as.matrix()
```

###Test Train Split

We will split the data into train/test sets before running cross validation.  This will allow us to make sure that predictions are consistent at all levels of PH.

```{r}
bev_eval <- bev_imputed[2567:2833,]
bev_train <- bev_imputed[1:2566,]
samples <- createDataPartition(y, p = .75, list = F)
x_train <- bev_train[samples, ]
x_test<- bev_train[-samples, ]

y_train <- y[samples]
y_test <- y[-samples]
```

##Build Models

For state-of-the-art prediction quality, we will use a model stack.  This will consist of tuning models separately and then combining the candidate models into a metamodel that will formulate predictions with a linear combination of our tuned models.  

From the perspective of understanding the manufacturing process, the model stack will also provide benefits.  The stack is like a panel of experts, each looking at the data through slightly different lenses to form their diagnoses.  By looking at the predictors each model uses, we can gather assemble a complete picture of the factors that affect our manufacturing process.  


```{r}
#xgb_grid <- expand.grid(eta = c(.01,.025), nrounds = c(750,1000), max_depth = c(5,6,7), gamma = c(.1,0), colsample_bytree = c(.8), min_child_weight = c(.4,.8), subsample = c(.8))
#cub_grid <- expand.grid(.committees = c(1,3,5), .neighbors = c(1,3,5,7,9))
#mars_grid <- expand.grid(.degree = c(1,2), .nprune = seq(16,36,4))
#dart_grid <- expand.grid(eta = c(.025), nrounds = c(1000), gamma = c(.001,.1), skip_drop = c(.4,.8), rate_drop = c(.4,.8), #max_depth = c(6,7,8), colsample_bytree = c(.8), min_child_weight = c(.8), subsample = c(.8))
#rf_grid <- expand.grid(mtry = c(5,10,15,20,25,30))

xgb_grid <- expand.grid(eta = c(.01), nrounds = c(1000), max_depth = c(6), gamma = c(0), colsample_bytree = c(.8), min_child_weight = c(.8), subsample = c(.8))
cub_grid <- expand.grid(.committees = c(5), .neighbors = c(7))
mars_grid <- expand.grid(.degree = c(2), .nprune = 24)
dart_grid <- expand.grid(eta = c(.01), nrounds = c(1000), gamma = c(.1), skip_drop = c(.6), rate_drop = c(.4), max_depth = c(6), colsample_bytree = c(.6), min_child_weight = c(.6), subsample = c(.6))
rf_grid <- expand.grid(mtry = 25)

tuning_list <-list(
  caretModelSpec(method="xgbTree", tuneGrid = xgb_grid),
  caretModelSpec(method="xgbDART", tuneGrid = dart_grid),
  caretModelSpec(method="cubist", tuneGrid = cub_grid),
  caretModelSpec(method="rf", tuneGrid = rf_grid, importance = TRUE),
  caretModelSpec(method="bagEarth", tuneGrid = mars_grid)
)

cl <- makeCluster(detectCores())
registerDoParallel(cl)
set.seed(42)

my_control <- trainControl(method = 'cv',
                           number = 5,
                           savePredictions = 'final',
                           index = createFolds(y_train, 5),
                           allowParallel = TRUE) 

mod_list <- caretList(
  x = x_train,
  y = y_train,
  preProcess = c('center', 'scale'),
  trControl = my_control,
  tuneList = tuning_list
)
```

*The training grid is commented out because of the time required to run, but can be uncommented for reproducibility of our model optimization*

###MARS

```{r }
#helper function
show_results <- function(model){ 
  rslts <- model$results %>%
    arrange(RMSE)
  head(rslts, 10) %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)
}
```

```{r}

show_results(mod_list$bagEarth)
#plot(mod_list$mars, metric = "RMSE")
```


```{r}
predict(mod_list$bagEarth, x_test) %>%
  postResample(y_test)
```

###RandomForest

```{r}
show_results(mod_list$rf)

```


```{r}
predict(mod_list$rf, x_test) %>%
  postResample(y_test)
```

###Cubist

```{r}
show_results(mod_list$cubist)
#plot(mod_list$cub, metric = "RMSE")
```


```{r}
predict(mod_list$cubist, x_test) %>%
  postResample(y_test)
```

###XGB Trees

```{r}
  
show_results(mod_list$xgbTree)
#plot(mod_list$xgbt, metric = "RMSE")
```


```{r}
predict(mod_list$xgbTree, x_test) %>%
  postResample(y_test)
```

###XGB Dart

```{r}
show_results(mod_list$xgbDART)
#plot(mod_list$xgbd, metric = "RMSE")
```

```{r}
predict(mod_list$xgbDART, x_test) %>%
  postResample(y_test)
```


**Assessment**

While the XGBTee model offers the lowest RMSE, it still only explains about 50% of the variance in PH.  There is plenty of room for the other models to offer value.  We will use the hyperparameters yielding the best RMSE in our final ensemble that will be built later.

###Variable Importances

```{r, fig.height = 8, fig.width=12}
p1 <- varImp(mod_list$cubist) %>%
  plot(top = 10, main = 'Cubist')
p2 <- varImp(mod_list$xgbTree) %>%
  plot(top = 10, main = 'XGBTrees')
p3 <- varImp(mod_list$xgbDART) %>%
  plot(top = 10, main = 'XGBDart')
p4 <- varImp(mod_list$bagEarth) %>%
  plot(top = 10, main = 'MARS')
p5 <- varImp(mod_list$rf) %>%
  plot(top = 10, main = 'RF')
grid.arrange(p1,p2,p3,p4,p5, nrow = 3, ncol = 2)
```

Observations:

- The top predictor, MnFlow, is consistent throughout the set of models
- The XGBoost models are similar in variables, but less similar in their distributions of importance
- After the top 5, divergence increases

Based on what we see, there are more factors contributing to PH with the ensemble than with a single model.  We'll confirm this by looking at the count by predictor among each top 10:


**Variable Counts among the Models**

```{r}
top_10 <- function(model) {
  varImp(model)$importance %>%
  rownames_to_column('var') %>%
  arrange(desc(Overall)) %>%
  head(10) %>%
  select(var) 
}

rbind(top_10(mod_list$xgbDART), top_10(mod_list$xgbTree), top_10(mod_list$cubist), top_10(mod_list$bagEarth), top_10(mod_list$rf)) %>%
  group_by(var) %>%
  summarise(ModelCount = n()) %>%
  arrange(desc(ModelCount)) %>%
  kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)

```


**Best Single Tree**

Because 4 of the 5 models in the ensemble are tree-based, looking a single decision tree model will give us an idea of not just what predictors are important, but how they are being used to form predictions.


```{r}
rp_mod <- rpart::rpart(ph ~ ., data = data.frame(bev_train, ph = y), method = 'anova')
rpart.plot(rp_mod, uniform=TRUE)
```

####Test Model Stack

Now that our models are optimized and understood, we will form the ensemble and test the results.  We first look at their prediction correlations to confirm that the stack, intuitively, be effective.  The lower the correlation between the models, the more effective the stack will be.  

```{r}

resamples <- resamples(mod_list)
modelCor(resamples) %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

*Our model list has negative correlations, indicating that it will be effective.*



```{r}
ensemble1 <- caretStack(
  mod_list,
  method="glmnet",
  metric="RMSE",
  trControl=trainControl(
    method="cv",
    number=5,
    savePredictions="final"
  )
)
ensemble1$ens_model$results %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

*We see a several percent improvement over the best single model.*  

```{r}
preds <- predict(ensemble1, x_test) 
preds %>% postResample(y_test)
```

**Predicted Vs Actual**

```{r}
preds_df <- data.frame(predicted = preds, actual = y_test)
ggplot(preds_df) + geom_point(aes(x = predicted, y = actual)) + geom_smooth(aes(x = predicted, y = actual))
```

##Build Models

On the test set, the ensemble's predictions are consistent throughout the range of PH.  Consistent quality is, theoretically, a feature of ensemble models because the models are weighted such that predictions are maximized.  If one model predicts better at lower PH, it will be upweighted over that range.  There are several outliers, but when not all of the variance is explainable, this is unavoidable.

##Predict on Eval Set

With our hyperparameters of the individual models optimized, we want to provide each model the entire training set and build the meta-model based on these optimized single models.


```{r}
xgb_grid <- expand.grid(eta = c(.01), nrounds = c(1000), max_depth = c(6), gamma = c(0), colsample_bytree = c(.8), min_child_weight = c(.8), subsample = c(.8))
cub_grid <- expand.grid(.committees = c(5), .neighbors = c(7))
mars_grid <- expand.grid(.degree = c(2), .nprune = 24)
dart_grid <- expand.grid(eta = c(.01), nrounds = c(1000), gamma = c(.1), skip_drop = c(.6), rate_drop = c(.4), max_depth = c(6), colsample_bytree = c(.6), min_child_weight = c(.6), subsample = c(.6))
rf_grid <- expand.grid(mtry = 25)

tuning_list <-list(
  caretModelSpec(method="xgbTree", tuneGrid = xgb_grid),
  caretModelSpec(method="xgbDART", tuneGrid = dart_grid),
  caretModelSpec(method="cubist", tuneGrid = cub_grid),
  caretModelSpec(method="rf", tuneGrid = rf_grid, importance = TRUE),
  caretModelSpec(method="bagEarth", tuneGrid = mars_grid)
)



my_control <- trainControl(method = 'cv',
                           number = 5,
                           savePredictions = 'final',
                           index = createFolds(y, 5),
                           allowParallel = TRUE) 

mod_list <- caretList(
  x = bev_train,
  y = y,
  preProcess = c('center', 'scale'),
  trControl = my_control,
  tuneList = tuning_list
)
final_ensemble <- caretStack(
  mod_list,
  method="glmnet",
  metric="RMSE",
  trControl=trainControl(
    method="cv",
    number=5,
    savePredictions="final"
  )
)
final_ensemble$ens_model$results %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)

```

*With more data available, the ensemble improve around 6%.  Our model is considerably more predictive than a "baseline" single model.*

**Ensemble Model Importances**

```{r}
plot(varImp(final_ensemble$ens_model))
```


*The MARS model is excluded, but all four tree models contribute the the final prediction*


**Export Predictions**

```{r}
predictions <- predict(final_ensemble, bev_eval)
predictions %>% tibble::enframe(name = NULL) %>% datatable()
write.csv(predictions, file = "predictions.csv")
```
